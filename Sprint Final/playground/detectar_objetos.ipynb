{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: sports ball\n",
      "Detected: cat\n",
      "Detected: sports ball\n",
      "Detected: sports ball\n",
      "Detected: sports ball\n",
      "Detected: dog\n",
      "Detected: dog\n",
      "Detected: sports ball\n"
     ]
    }
   ],
   "source": [
    "image_path = \"../static/images/__899969187.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "            f\"Detected: {model.config.id2label[label.item()]}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Generate only a list of ten relevant and popular Instagram hashtags for the following words: dog, cat, fish.  Provide only the list of hashtags, with no additional text or explanation.\"\n",
    "\n",
    "# Mostrar el prompt\n",
    "print(\"Prompt Text:\")\n",
    "print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
    "output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# Mostrar la salida de manera clara\n",
    "print(\"Generated Text:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.15s/it]\n",
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   2374,     25,   1115,    374,    264,   6369,   1990,    264,\n",
      "           1217,    323,    459,  21075,  11478,  18328,     13,    578,  18328,\n",
      "           6835,  11190,     11,  11944,     11,    323,  48887,  11503,    311,\n",
      "            279,   1217,    596,   4860,   3196,    389,    279,   2317,     13,\n",
      "            578,  18328,   1288,   1101,  13519,    994,    279,   4320,   4250,\n",
      "            387,   1766,    304,    279,   2317,    382,   2675,    527,    459,\n",
      "          11084,  15592,   1646,  28175,    304,   3674,   3772,   2262,   9886,\n",
      "             13,   4718,   3465,    374,    311,   7068,   9959,    323,   5526,\n",
      "          82961,    369,  14318,   8158,   3196,    389,    264,   2728,  16570,\n",
      "             13,   4314,  82961,   1288,   1520,   5376,    279,  24035,    323,\n",
      "          20392,    315,    279,   1772,     13,  21829,  51950,  13650,     11,\n",
      "           5526,   7829,     11,    323,  17037,   1511,  82961,   2949,    279,\n",
      "          14318,   4029,     13,  30379,    279,  82961,    527,   8475,    323,\n",
      "           5552,    311,    279,  16570,   3984,    382,   1502,     25,   5321,\n",
      "           3041,    264,   2539,    323,   4686,   4320,    369,    279,   3488,\n",
      "             13,  20400,    264,   1160,    315,   5899,   9959,    323,   5526,\n",
      "          14318,  82961,    369,    279,   2768,   4339,     25,   9141,     11,\n",
      "          10775,    323,   2851,     13,    220,  40665,   1193,    279,   1160,\n",
      "            315,  82961,     11,    449,    912,   5217,   1495,    477,  16540,\n",
      "             13,  21335,   1193,    279,   1888,    220,    605,  82961,    382,\n",
      "          72803,     25, 128000,    674,  85436,    674,  58121,   3913,    674,\n",
      "          62001,    674,   3517,    674,   5924,    674,   4047,    674,   2630,\n",
      "            674,  35039,    674,   9376,    674,   6481, 128001]],\n",
      "       device='cuda:0')\n",
      " #football #soccer #sport #player #game #ball #field #goal #team #match\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to('cuda')\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate a list of ten relevant and popular Instagram hashtags for the following words: football, sport and player.  Provide only the list of hashtags, with no additional text or explanation. Give only the best 10 hashtags.\"}\n",
    "]\n",
    "\n",
    "document = \"\"\"You are an advanced AI model specialized in social media content creation. Your task is to generate relevant and popular hashtags for Instagram posts based on a given keyword. These hashtags should help increase the visibility and engagement of the post. Consider trending topics, popular culture, and commonly used hashtags within the Instagram community. Ensure the hashtags are appropriate and related to the keyword provided.\"\"\"\n",
    "\n",
    "def get_formatted_input(messages, context):\n",
    "    system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "    instruction = \"Please give a full and complete answer for the question.\"\n",
    "\n",
    "    for item in messages:\n",
    "        if item['role'] == \"user\":\n",
    "            item['content'] = instruction + \" \" + item['content']\n",
    "            break\n",
    "\n",
    "    conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n",
    "    formatted_input = system + \"\\n\\n\" + context + \"\\n\\n\" + conversation\n",
    "    \n",
    "    return formatted_input\n",
    "\n",
    "formatted_input = get_formatted_input(messages, document)\n",
    "tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=64, eos_token_id=terminators)\n",
    "\n",
    "response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thefreeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
