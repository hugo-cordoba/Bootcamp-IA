{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/emoji_sentiment_dataset.csv')\n",
    "\n",
    "print(df)\n",
    "print(f\"El dataset tiene {df.shape[0]} filas\")\n",
    "print(f\"El dataset tiene {df.isnull().any(axis=1).sum()} filas con valores nulos\")\n",
    "df_clean = df.dropna()\n",
    "print(f\"El dataset tiene {df_clean.isnull().any(axis=1).sum()} filas con valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby('category').count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_word_cloud(terms, category):\n",
    "    # Definir un t√≠tulo basado en el valor de la categor√≠a\n",
    "    if category == 1.0:\n",
    "        title = \"Positivo\"\n",
    "    elif category == 0.0:\n",
    "        title = \"Neutral\"\n",
    "    else:\n",
    "        title = \"Negativo\"\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(terms)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Nube de Palabras - Sentimiento {title}', fontsize=20)  # A√±adir t√≠tulo aqu√≠\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Aseg√∫rate de que los datos est√°n limpios y los textos son strings\n",
    "df_clean['clean_text'] = df_clean['clean_text'].astype(str)\n",
    "\n",
    "# Generar nubes de palabras para cada categor√≠a\n",
    "for category in [-1.0, 0.0, 1.0]:\n",
    "    subset = df_clean[df_clean['category'] == category]\n",
    "    texts = \" \".join(subset['clean_text'])\n",
    "    tfidf_vector = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    X_sub = tfidf_vector.fit_transform([texts])\n",
    "    max_words = {word: X_sub[0, idx] for word, idx in tfidf_vector.vocabulary_.items()}\n",
    "    plot_word_cloud(max_words, category)  # Pasar tambi√©n la categor√≠a como argumento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizaci√≥n\n",
    "tokenizer = Tokenizer(num_words=5000)  # Solo considera las 5000 palabras m√°s frecuentes\n",
    "tokenizer.fit_on_texts(df_clean['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df_clean['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "max_sequence_len = max([len(x) for x in sequences])\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
    "y = to_categorical(np.asarray(df_clean['category'] + 1))  # Convertir -1, 0, 1 a 0, 1, 2 para categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_sequence_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dense(3, activation='softmax')  # 3 categor√≠as de salida\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, validation_split=0.1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(model.evaluate(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para obtener el nombre Unicode de un emoji\n",
    "def emoji_to_unicode_name(em):\n",
    "    return emoji.demojize(em)\n",
    "\n",
    "# Lista de emojis\n",
    "new_text = 'üòçüòçü§òü§ò‚ù§Ô∏è‚ù§Ô∏èüëÑüòéüòé'\n",
    "\n",
    "# Convertir la lista a un pandas Series\n",
    "new_text_series = pd.Series(new_text)\n",
    "\n",
    "# Aplicar la funci√≥n a la Series\n",
    "new_text_unicode_names = new_text_series.apply(emoji_to_unicode_name).str.replace(':',' ')\n",
    "\n",
    "\n",
    "# Convertir de nuevo a lista si es necesario\n",
    "new_text = new_text_unicode_names.tolist()\n",
    "\n",
    "print(new_text)\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(new_text)\n",
    "padded = pad_sequences(seq, maxlen=max_sequence_len)\n",
    "\n",
    "# Realizar la predicci√≥n\n",
    "pred = model.predict(padded)\n",
    "print(f'Predicci√≥n: {pred[0]}')  # Muestra la probabilidad de cada categor√≠a\n",
    "\n",
    "# Determinar la categor√≠a m√°s probable\n",
    "predicted_category_index = np.argmax(pred[0])\n",
    "categories = {-1: 'Negativo', 0: 'Neutral', 1: 'Positivo'}  # Categor√≠as ajustadas para corresponder a tus etiquetas\n",
    "predicted_category = categories[predicted_category_index - 1]  # ajustar el √≠ndice para -1, 0, 1\n",
    "print(f'La categor√≠a m√°s probable es: {predicted_category} con una probabilidad de {pred[0][predicted_category_index]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Generate only a list of ten relevant and popular Instagram hashtags for the following words: dog, cat, fish.  Provide only the list of hashtags, with no additional text or explanation.\"\n",
    "\n",
    "# Mostrar el prompt\n",
    "print(\"Prompt Text:\")\n",
    "print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
    "output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# Mostrar la salida de manera clara\n",
    "print(\"Generated Text:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.34s/it]\n",
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\hugoc\\anaconda3\\envs\\thefreeai\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #football #soccer #footballplayer #footballgame #footballmatch #footballfans #footballseason #footballtraining #footballteam #footballstar\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate a list of ten relevant and popular Instagram hashtags for the following words: football, sport and player.  Provide only the list of hashtags, with no additional text or explanation. Give only the best 10 hashtags.\"}\n",
    "]\n",
    "\n",
    "def get_formatted_input(messages):\n",
    "    system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "    instruction = \"Please give a full and complete answer for the question.\"\n",
    "\n",
    "    for item in messages:\n",
    "        if item['role'] == \"user\":\n",
    "            ## only apply this instruction for the first user turn\n",
    "            item['content'] = instruction + \" \" + item['content']\n",
    "            break\n",
    "\n",
    "    conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n",
    "    formatted_input = system + \"\\n\\n\" + conversation\n",
    "    \n",
    "    return formatted_input\n",
    "\n",
    "formatted_input = get_formatted_input(messages)\n",
    "tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=64, eos_token_id=terminators)\n",
    "\n",
    "response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thefreeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
